{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">使用 CUDA C/C++ 统一内存和 Nsight Systems (nsys) 管理加速应用程序内存</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于本实验和其他 CUDA 基础实验，我们强烈建议您遵循 [*CUDA 最佳实践指南*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)，其中推荐一种称为 **APOD** 的设计周期：**评估**、**并行化**、**优化**和**部署**。简言之，APOD 规定一个迭代设计过程，开发人员能够在该过程中对其加速应用程序性能施以渐进式改进，并发布代码。随着开发人员的 CUDA 编程能力愈渐增强，他们已能在加速代码库中应用更先进的优化技术。\n",
    "\n",
    "本实验将支持这种迭代开发风格。您将使用 **Nsight Systems命令行分析器**定性衡量应用程序性能及确定优化机会，之后您将应用渐进式改进，最后您会学习新技术并重复该周期。需重点关注的是，您将在本实验中学习及应用的众多技术均会涉及 CUDA **统一内存**工作原理的具体细节。理解统一内存行为是 CUDA 开发人员的一项基本技能，同时也可作为多项更先进内存管理技术的先决条件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 预备知识\n",
    "\n",
    "如要充分利用本实验，您应已能胜任如下任务：\n",
    "\n",
    "- 编写、编译及运行既可调用 CPU 函数也可启动 GPU 核函数的 C/C++ 程序。\n",
    "- 使用执行配置控制并行线程层次结构。\n",
    "- 重构串行循环以在 GPU 上并行执行其迭代。\n",
    "- 分配和释放统一内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 学习目标\n",
    "\n",
    "当您在本实验完成学习后，您将能够：\n",
    "\n",
    "- 使用 **Nsight Systems命令行分析器** (**nsys**) 分析被加速的应用程序的性能。\n",
    "- 利用对**流多处理器**的理解优化执行配置。\n",
    "- 理解**统一内存**在页错误和数据迁移方面的行为。\n",
    "- 使用**异步内存预取**减少页错误和数据迁移以提高性能。\n",
    "- 采用循环式的迭代开发加快应用程序的优化加速和部署。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 使用nsys性能分析器帮助应用程序迭代地进行优化\n",
    "\n",
    "如要确保优化加速代码库的尝试真正取得成功，唯一方法便是分析应用程序以获取有关其性能的定量信息。`nsys` 是指 NVIDIA 的Nsight System命令行分析器。该分析器附带于CUDA工具包中，提供分析被加速的应用程序性能的强大功能。\n",
    "\n",
    "`nsys` 使用起来十分简单，最基本用法是向其传递使用 `nvcc` 编译的可执行文件的路径。随后 `nsys` 会继续执行应用程序，并在此之后打印应用程序 GPU 活动的摘要输出、CUDA API 调用以及**统一内存**活动的相关信息。我们稍后会在本实验中详细介绍这一主题。\n",
    "\n",
    "在加速应用程序或优化已经加速的应用程序时，我们应该采用科学的迭代方法。作出更改后需分析应用程序、做好记录并记录任何重构可能会对性能造成何种影响。尽早且经常进行此类观察通常会让您轻松获得足够的性能提升，以助您发布加速应用程序。此外，经常分析应用程序将使您了解到对 CUDA 代码库作出的特定更改会对其实际性能造成何种影响：而当只在代码库中进行多种更改后再分析应用程序时，将很难得知这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 练习：使用nsys分析应用程序\n",
    "\n",
    "[01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu)（<------您可点击打开此文件链接和本实验中的任何源文件链接并进行编辑）是一个简单易用的加速向量加法程序。使用下方两个代码执行单元（按住 `CTRL` 并点击即可）。第一个代码执行单元将编译（及运行）向量加法程序。第二个代码执行单元将运用 `nsys profile` 分析刚编译好的可执行文件。\n",
    "\n",
    "`nsys profile`将生成一个`qdrep`报告文件，该文件可以以多种方式使用。 我们在这里使用`--stats = true`标志表示我们希望打印输出摘要统计信息。 输出的信息有很多，包括：\n",
    "\n",
    "- 配置文件配置详细信息\n",
    "- 报告文件的生成详细信息\n",
    "- **CUDA API统计信息**\n",
    "- **CUDA核函数的统计信息**\n",
    "- **CUDA内存操作统计信息（时间和大小）**\n",
    "- 操作系统内核调用接口的统计信息\n",
    "\n",
    "在本实验中，您将主要使用上面**黑体字**的3个部分。 在下一个实验中，您将使用生成的报告文件将其提供给Nsight Systems 进行可视化分析。\n",
    "\n",
    "\n",
    "应用程序分析完毕后，请使用分析输出中显示的信息回答下列问题：\n",
    "\n",
    "- 此应用程序中唯一调用的 CUDA 核函数的名称是什么？\n",
    "- 此核函数运行了多少次？\n",
    "- 此核函数的运行时间为多少？请记录下此时间：您将继续优化此应用程序，再和此时间做对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-a738-4d23-7e7a-2719.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-a738-4d23-7e7a-2719.qdrep\"\n",
      "Exporting 4558 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-a738-4d23-7e7a-2719.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average      Minimum     Maximum            Name         \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  ---------------------\n",
      "    89.2       2355241188          1  2355241188.0  2355241188  2355241188  cudaDeviceSynchronize\n",
      "     9.8        258038072          3    86012690.7       18445   257971990  cudaMallocManaged    \n",
      "     1.0         26419363          3     8806454.3     7823574    10403440  cudaFree             \n",
      "     0.0            52639          1       52639.0       52639       52639  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average      Minimum     Maximum                       Name                    \n",
      " -------  ---------------  ---------  ------------  ----------  ----------  -------------------------------------------\n",
      "   100.0       2355229957          1  2355229957.0  2355229957  2355229957  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.6         68517536        2304  29738.5     1855   181920  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.4         20928032         768  27250.0     1119   167040  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    58.7       5377926550        277  19414897.3    20148  100131348  poll          \n",
      "    39.8       3645180604        278  13112160.4    13419  100077175  sem_timedwait \n",
      "     1.1        102929134        592    173866.8     1065   20996358  ioctl         \n",
      "     0.3         28538850         90    317098.3     1200   10329393  mmap          \n",
      "     0.0           608424         77      7901.6     2107      18108  open64        \n",
      "     0.0           108164         23      4702.8     1338      15340  fopen         \n",
      "     0.0           107276          4     26819.0    24688      30471  pthread_create\n",
      "     0.0            98656          3     32885.3    20488      55385  fgets         \n",
      "     0.0            78887         11      7171.5     3985      11138  write         \n",
      "     0.0            44471         14      3176.5     1225       5252  munmap        \n",
      "     0.0            32156          5      6431.2     2573      10896  open          \n",
      "     0.0            31297         16      1956.1     1040       3419  fclose        \n",
      "     0.0            24906         12      2075.5     1064       3082  read          \n",
      "     0.0            12188          3      4062.7     3975       4123  pipe2         \n",
      "     0.0             9034          2      4517.0     3829       5205  socket        \n",
      "     0.0             8066          2      4033.0     1336       6730  fgetc         \n",
      "     0.0             6942          2      3471.0     3019       3923  fread         \n",
      "     0.0             6187          4      1546.8     1475       1610  mprotect      \n",
      "     0.0             5113          1      5113.0     5113       5113  connect       \n",
      "     0.0             4301          1      4301.0     4301       4301  fcntl         \n",
      "     0.0             2175          1      2175.0     2175       2175  bind          \n",
      "     0.0             1337          1      1337.0     1337       1337  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report1.qdrep\"\n",
      "Report file moved to \"/dli/task/report1.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值得一提的是，默认情况下，`nsys profile`不会覆盖现有的报告文件。 这样做是为了防止在进行概要分析时意外丢失工作。 如果出于某种原因，您宁愿覆盖现有的报告文件，例如在快速迭代期间，可以向`nsys profile`提供`-f`标志以允许覆盖现有的报告文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：优化并分析性能\n",
    "\n",
    "请抽出一到两分钟时间，更新 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 的执行配置以对其进行简单的优化，以便其能在单个线程块中的多个线程上运行。请使用下方的代码执行单元重新编译并借助 `nsys profile` 进行分析。使用性能分析的输出检查核函数的运行时间。此次优化带来多大的速度提升？请务必在某处记录下您的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-c87c-e22c-5145-08e0.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-c87c-e22c-5145-08e0.qdrep\"\n",
      "Exporting 4150 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-c87c-e22c-5145-08e0.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    55.1        226287749          3   75429249.7      19636  226221774  cudaMallocManaged    \n",
      "    38.4        157871757          1  157871757.0  157871757  157871757  cudaDeviceSynchronize\n",
      "     6.5         26583420          3    8861140.0    8086354   10214354  cudaFree             \n",
      "     0.0            58748          1      58748.0      58748      58748  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        157859438          1  157859438.0  157859438  157859438  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.7         68626336        2304  29785.7     1887   176096  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.3         20876736         768  27183.3     1151   159744  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2304  170.667    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    53.5       1354839478         74  18308641.6    33780  100130612  poll          \n",
      "    41.9       1060589196         74  14332286.4    10936  100068861  sem_timedwait \n",
      "     3.4         86747088        590    147029.0     1001   19792997  ioctl         \n",
      "     1.2         29552139         90    328357.1     1162   10145973  mmap          \n",
      "     0.0           649653         77      8437.1     2788      19871  open64        \n",
      "     0.0           126449          4     31612.3    27556      34655  pthread_create\n",
      "     0.0           112625         23      4896.7     1454      15648  fopen         \n",
      "     0.0           111626          3     37208.7    25728      57384  fgets         \n",
      "     0.0            88803         11      8073.0     4812      12782  write         \n",
      "     0.0            47642         16      2977.6     1178       4250  munmap        \n",
      "     0.0            36909          5      7381.8     3212      10171  open          \n",
      "     0.0            30605         16      1912.8     1116       3286  fclose        \n",
      "     0.0            25193         11      2290.3     1030       4429  read          \n",
      "     0.0            14068          3      4689.3     4519       4987  pipe2         \n",
      "     0.0            12112          2      6056.0     5352       6760  socket        \n",
      "     0.0             8490          2      4245.0     1656       6834  fgetc         \n",
      "     0.0             7164          4      1791.0     1723       1827  mprotect      \n",
      "     0.0             6552          1      6552.0     6552       6552  connect       \n",
      "     0.0             6377          2      3188.5     2857       3520  fread         \n",
      "     0.0             3562          1      3562.0     3562       3562  fcntl         \n",
      "     0.0             2318          1      2318.0     2318       2318  bind          \n",
      "     0.0             1566          1      1566.0     1566       1566  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report10.qdrep\"\n",
      "Report file moved to \"/dli/task/report10.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：迭代优化\n",
    "\n",
    "在本练习中，您将进行多轮周期式的迭代优化，具体包括：编辑 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 的执行配置、开展性能分析及记录结果以查看影响。开展操作时请依循以下指南：\n",
    "\n",
    "- 首先列出您将用于更新执行配置的 3 至 5 种不同方法，确保涵盖一系列不同的网格和线程块大小组合。\n",
    "- 使用所列的其中一种方法编辑 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 程序。\n",
    "- 使用下方的两个代码执行单元编译和分析更新后的代码。\n",
    "- 记录核函数执行的运行时间，应与性能分析输出中给出的时间一样。\n",
    "- 对以上列出的每个可能实现的优化重复执行编辑代码/性能分析/记录结果\n",
    "\n",
    "在您尝试的执行配置中，哪个经证明是最快的？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-cc19-c52e-3175-608a.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-cc19-c52e-3175-608a.qdrep\"\n",
      "Exporting 4717 events: [==================================================100%]17%                                             ]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-cc19-c52e-3175-608a.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    55.6        208378719          3   69459573.0      21060  208309556  cudaMallocManaged    \n",
      "    37.3        139704471          1  139704471.0  139704471  139704471  cudaDeviceSynchronize\n",
      "     7.0         26370424          3    8790141.3    7796366   10458092  cudaFree             \n",
      "     0.0           122454          1     122454.0     122454     122454  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        139759651          1  139759651.0  139759651  139759651  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    76.9         69628868        2876  24210.3     1695   175904  [CUDA Unified Memory memcpy HtoD]\n",
      "    23.1         20939776         768  27265.3     1151   178304  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        2876  136.723    4.000   988.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    54.5       1344966360         73  18424196.7    21837  100139275  poll          \n",
      "    41.1       1014362676         72  14088370.5    10066  100070300  sem_timedwait \n",
      "     3.2         78513133        589    133299.0     1005   17543322  ioctl         \n",
      "     1.2         28620877         90    318009.7     1145   10389952  mmap          \n",
      "     0.0           588589         77      7644.0     2332      17932  open64        \n",
      "     0.0           117989          4     29497.3    26000      35406  pthread_create\n",
      "     0.0           103660         23      4507.0     1281      14960  fopen         \n",
      "     0.0            98882          3     32960.7    20465      44566  fgets         \n",
      "     0.0            83823         11      7620.3     3840      11012  write         \n",
      "     0.0            44430         14      3173.6     1220       5161  munmap        \n",
      "     0.0            31595          5      6319.0     2600       8952  open          \n",
      "     0.0            28404         16      1775.3     1079       3447  fclose        \n",
      "     0.0            23484         11      2134.9     1017       2860  read          \n",
      "     0.0            11594          3      3864.7     3365       4603  pipe2         \n",
      "     0.0             8583          2      4291.5     3637       4946  socket        \n",
      "     0.0             8164          2      4082.0     1466       6698  fgetc         \n",
      "     0.0             6805          4      1701.3     1494       2045  mprotect      \n",
      "     0.0             5615          2      2807.5     2217       3398  fread         \n",
      "     0.0             5534          1      5534.0     5534       5534  connect       \n",
      "     0.0             5214          2      2607.0     1048       4166  fcntl         \n",
      "     0.0             1889          1      1889.0     1889       1889  bind          \n",
      "     0.0             1343          1      1343.0     1343       1343  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report12.qdrep\"\n",
      "Report file moved to \"/dli/task/report12.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 流多处理器（Streaming Multiprocessors）及查询GPU的设备配置\n",
    "\n",
    "本节将探讨了解 GPU 硬件的特定功能，以进一步促进优化。学习完**流多处理器**后，您将尝试进一步优化自己一直执行的加速向量加法程序。\n",
    "\n",
    "以下幻灯片以可视化的方式概要地介绍了后面将学习的内容。 在更详细地学习这些内容之前，请单击每一页幻灯片直至结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_UM_NVPROF-zh/NVPROF_UM_1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_UM_NVPROF-zh/NVPROF_UM_1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流多处理器和Warps\n",
    "\n",
    "运行 CUDA 应用程序的 GPU 具有称为**流多处理器**（或 **SM**）的处理单元。在核函数执行期间，将线程块提供给 SM 以供其执行。为支持 GPU 执行尽可能多的并行操作，您通常可以*选择线程块数量数倍于指定 GPU 上 SM 数量的网格大小*来提升性能。\n",
    "\n",
    "此外，SM 会在一个名为**warp**的线程块内创建、管理、调度和执行包含 32 个线程的线程组。本课程将不会更[深入探讨 SM 和warp](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)，但值得注意的是，您也可*选择线程数量数倍于 32 的线程块大小*来提升性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以编程方式查询GPU设备属性\n",
    "\n",
    "由于 GPU 上的 SM 数量会因所用的特定 GPU 而异，因此为支持可移植性，您不得将 SM 数量硬编码到代码库中。相反，应该以编程方式获取此信息。\n",
    "\n",
    "以下所示为在 CUDA C/C++ 中获取 C 结构的方法，该结构包含当前处于活动状态的 GPU 设备的多个属性，其中包括设备的 SM 数量：\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习: 查询设备信息\n",
    "\n",
    "目前，[`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) 包含众多未分配的变量，并将打印一些无用信息，这些信息用于描述当前处于活动状态的 GPU 设备的详细信息。\n",
    "\n",
    "构建 [`01-get-device-properties.cu`](../edit/04-device-properties/01-get-device-properties.cu) 以打印源代码中指示的所需设备属性的实际值。为获取操作支持并查看相关介绍，请参阅 [CUDA 运行时文档](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) 以帮助识别设备属性结构中的相关属性。如您遇到问题，请参阅 [解决方案](../edit/04-device-properties/solutions/01-get-device-properties-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\r\n",
      "Number of SMs: 40\r\n",
      "Compute Capability Major: 7\r\n",
      "Compute Capability Minor: 5\r\n",
      "Warp Size: 32\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：将网格数调整为SM数，进一步优化矢量加法\n",
    "\n",
    "通过查询设备的 SM 数量重构您一直在 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 内执行的 `addVectorsInto` 核函数，以便其启动时的网格包含数倍于设备上 SM 数量的线程块数。\n",
    "\n",
    "根据您所编写代码中的其他特定详细信息，此重构可能会或不会提高或大幅改善核函数的性能。因此，请务必始终使用 `nsys profile`，以便定量评估性能变化。根据分析输出，记录目前所得结果和其他发现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-7230-bea0-c249-b1ff.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-7230-bea0-c249-b1ff.qdrep\"\n",
      "Exporting 8579 events: [==================================================99% ]==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-7230-bea0-c249-b1ff.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    57.0        208947730          3   69649243.3      19285  208881457  cudaMallocManaged    \n",
      "    35.8        131463293          1  131463293.0  131463293  131463293  cudaDeviceSynchronize\n",
      "     7.2         26431918          3    8810639.3    7640629   10973760  cudaFree             \n",
      "     0.0            51700          1      51700.0      51700      51700  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        131453780          1  131453780.0  131453780  131453780  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    78.4         76108196        6737  11297.0     1823   167936  [CUDA Unified Memory memcpy HtoD]\n",
      "    21.6         20943936         768  27270.8     1151   159744  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        6737   58.367    4.000   992.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    54.7       1333892037         72  18526278.3    21809  100141273  poll          \n",
      "    40.8        994429426         71  14006048.3    10383  100072058  sem_timedwait \n",
      "     3.2         79070398        589    134245.2     1008   17655798  ioctl         \n",
      "     1.2         28716022         90    319066.9     1161   10911192  mmap          \n",
      "     0.0           585653         77      7605.9     2257      17595  open64        \n",
      "     0.0           112871         11     10261.0     4079      40337  write         \n",
      "     0.0           111336          4     27834.0    25117      31169  pthread_create\n",
      "     0.0            97860         23      4254.8     1114      13400  fopen         \n",
      "     0.0            86873          3     28957.7    19264      43826  fgets         \n",
      "     0.0            54526         15      3635.1     1328      12073  munmap        \n",
      "     0.0            31093          5      6218.6     2792       8649  open          \n",
      "     0.0            28218         16      1763.6     1044       3707  fclose        \n",
      "     0.0            25533         13      1964.1     1025       3254  read          \n",
      "     0.0            12084          3      4028.0     3662       4429  pipe2         \n",
      "     0.0             9764          2      4882.0     4572       5192  socket        \n",
      "     0.0             8056          2      4028.0     1552       6504  fgetc         \n",
      "     0.0             6410          1      6410.0     6410       6410  connect       \n",
      "     0.0             6320          4      1580.0     1464       1712  mprotect      \n",
      "     0.0             5780          2      2890.0     2365       3415  fread         \n",
      "     0.0             4715          2      2357.5     1217       3498  fcntl         \n",
      "     0.0             1936          1      1936.0     1936       1936  bind          \n",
      "     0.0             1386          1      1386.0     1386       1386  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report16.qdrep\"\n",
      "Report file moved to \"/dli/task/report16.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 获得统一内存的细节\n",
    "\n",
    "您一直使用 `cudaMallocManaged` 分配旨在供主机或设备代码使用的内存，并且现在仍在享受这种方法的便利之处，即在实现自动内存迁移且简化编程的同时，而无需深入了解 `cudaMallocManaged` 所分配**统一内存** (**UM**) 实际工作原理的详细信息。`nsys profile` 提供有关加速应用程序中 UM 管理的详细信息，并在利用这些信息的同时结合对 UM 工作原理的更深入理解，进而为优化加速应用程序创造更多机会。\n",
    "\n",
    "以下幻灯片将直观呈现即将发布的材料的概要信息。点击浏览一遍这些幻灯片，然后再继续深入了解以下章节中的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_UM_NVPROF-zh/NVPROF_UM_2-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_UM_NVPROF-zh/NVPROF_UM_2-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统一内存(UM)的迁移\n",
    "\n",
    "分配 UM 时，内存尚未驻留在主机或设备上。主机或设备尝试访问内存时会发生 [页错误](https://en.wikipedia.org/wiki/Page_fault)，此时主机或设备会批量迁移所需的数据。同理，当 CPU 或加速系统中的任何 GPU 尝试访问尚未驻留在其上的内存时，会发生页错误并触发迁移。\n",
    "\n",
    "能够执行页错误并按需迁移内存对于在加速应用程序中简化开发流程大有助益。此外，在处理展示稀疏访问模式的数据时（例如，在应用程序实际运行之前无法得知需要处理的数据时），以及在具有多个 GPU 的加速系统中，数据可能由多个 GPU 设备访问时，按需迁移内存将会带来显著优势。\n",
    "\n",
    "有些情况下（例如，在运行时之前需要得知数据，以及需要大量连续的内存块时），我们还能有效规避页错误和按需数据迁移所产生的开销。\n",
    "\n",
    "本实验的后续内容将侧重于对按需迁移的理解，以及如何在分析器输出中识别按需迁移。这些知识可让您在享受按需迁移优势的同时，减少其产生的开销。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：探索统一内存（UM）的页错误\n",
    "\n",
    "`nsys profile` 会提供描述所分析应用程序 UM 行为的输出。在本练习中，您将对一个简单的应用程序做出一些修改，并会在每次更改后利用 `nsys profile` 的统一内存输出部分，探讨 UM 数据迁移的行为方式。\n",
    "\n",
    "[`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu) 包含 `hostFunction` 和 `gpuKernel` 函数，我们可以通过这两个函数并使用数字 `1` 初始化 `2<<24` 个单元向量的元素。主机函数和 GPU 核函数目前均未使用。\n",
    "\n",
    "对于以下 4 个问题中的每一问题，请根据您对 UM 行为的理解，首先假设应会发生何种页错误，然后使用代码库中所提供 2 个函数中的其中一个或同时使用这两个函数编辑 [`01-page-faults.cu`](../edit/06-unified-memory-page-faults/01-page-faults.cu)以创建场景，以便您测试假设。\n",
    "\n",
    "为了检验您的假设，请使用下面的代码执行单元来编译和分析代码。 一定要记录从`nsys profile --stats = true`输出中获得的假设以及结果。 在`nsys profile --stats = true`的输出中，您应该查找以下内容：\n",
    "\n",
    "- 输出中是否有 _CUDA内存操作统计信息_ 部分？\n",
    "- 如果是，这是否表示数据从主机到设备（HtoD）或从设备到主机（DtoH）的迁移？\n",
    "- 进行迁移时，输出如何说明有多少个“操作”？ 如果看到许多小的内存迁移操作，则表明按需出现页面错误，并且每次在请求的位置出现页面错误时都会发生小内存迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是供您探索的方案，以及遇到困难时的解决方案：\n",
    "\n",
    "- 当仅通过CPU访问统一内存时，是否存在内存迁移和/或页面错误的证据？（[解决方案](../edit/06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu)）\n",
    "- 当仅通过GPU访问统一内存时，是否有证据表明内存迁移和/或页面错误？（[解决方案](../edit/06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu)）\n",
    "- 当先由CPU然后由GPU访问统一内存时，是否有证据表明存在内存迁移和/或页面错误？（[解决方案](../edit/06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu)）\n",
    "- 当先由GPU然后由CPU访问统一内存时，是否存在内存迁移和/或页面错误的证据？ （[解决方案](../edit/06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu)）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-22f6-96fd-5f06-692a.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-22f6-96fd-5f06-692a.qdrep\"\n",
      "Exporting 1737 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-22f6-96fd-5f06-692a.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    89.6        215241799          1  215241799.0  215241799  215241799  cudaMallocManaged    \n",
      "     6.3         15138770          1   15138770.0   15138770   15138770  cudaDeviceSynchronize\n",
      "     4.1          9788510          1    9788510.0    9788510    9788510  cudaFree             \n",
      "     0.0            37090          1      37090.0      37090      37090  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum            Name          \n",
      " -------  ---------------  ---------  ----------  --------  --------  -----------------------\n",
      "   100.0         15132812          1  15132812.0  15132812  15132812  deviceKernel(int*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21163232         768  27556.3     1599   179488  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    48.9        581149445         33  17610589.2    19139  100118541  poll          \n",
      "    42.9        509313348         32  15916042.1     9912  100064976  sem_timedwait \n",
      "     7.1         84696828        578    146534.3     1204   17693783  ioctl         \n",
      "     1.0         12234546         84    145649.4     1081    9639921  mmap          \n",
      "     0.1           607903         77      7894.8     2196      20037  open64        \n",
      "     0.0           118181          4     29545.3    27521      32805  pthread_create\n",
      "     0.0           101998         23      4434.7     1269      13699  fopen         \n",
      "     0.0            86942          3     28980.7    20127      43667  fgets         \n",
      "     0.0            76582         11      6962.0     1766      11588  write         \n",
      "     0.0            31924          4      7981.0     1717      17215  fgetc         \n",
      "     0.0            31310          5      6262.0     2737       8597  open          \n",
      "     0.0            28848         10      2884.8     1001       4105  munmap        \n",
      "     0.0            27059         16      1691.2     1010       2712  fclose        \n",
      "     0.0            23813         10      2381.3     1036       3569  read          \n",
      "     0.0            12618          3      4206.0     3796       4958  pipe2         \n",
      "     0.0             9224          2      4612.0     3668       5556  socket        \n",
      "     0.0             7068          4      1767.0     1397       2322  mprotect      \n",
      "     0.0             6790          2      3395.0     2918       3872  fread         \n",
      "     0.0             5491          1      5491.0     5491       5491  connect       \n",
      "     0.0             5252          2      2626.0     1025       4227  fcntl         \n",
      "     0.0             2218          1      2218.0     2218       2218  bind          \n",
      "     0.0             1333          1      1333.0     1333       1333  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report19.qdrep\"\n",
      "Report file moved to \"/dli/task/report19.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：重新审视矢量加法程序的UM行为\n",
    "\n",
    "返回您一直在本实验中执行的 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 程序，查看当前状态的代码库，并假设您预期会发生哪种类型的内存迁移和/或页面错误。 查看最后一次重构的概要分析输出（通过向上滚动查找输出或通过执行下面的代码执行单元），并观察性能分析器输出的 _CUDA内存操作统计信息_ 部分。 您能否根据代码库的内容解释迁移的种类及其操作的数量？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-47dc-9780-8795-c507.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-47dc-9780-8795-c507.qdrep\"\n",
      "Exporting 8450 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-47dc-9780-8795-c507.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    58.9        226816529          3   75605509.7      17223  226762304  cudaMallocManaged    \n",
      "    34.2        131454024          1  131454024.0  131454024  131454024  cudaDeviceSynchronize\n",
      "     6.9         26488930          3    8829643.3    7745672   10632539  cudaFree             \n",
      "     0.0            47928          1      47928.0      47928      47928  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances    Average     Minimum    Maximum                      Name                    \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  -------------------------------------------\n",
      "   100.0        131443354          1  131443354.0  131443354  131443354  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    78.3         76356739        6613  11546.5     1823   169280  [CUDA Unified Memory memcpy HtoD]\n",
      "    21.7         21208864         768  27615.7     1599   169440  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        6613   59.461    4.000  1012.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    53.9       1333483484         72  18520603.9    18094  100126231  poll          \n",
      "    41.0       1014566694         71  14289671.7     9892  100068626  sem_timedwait \n",
      "     3.9         97054899        587    165340.5     1066   17779872  ioctl         \n",
      "     1.2         28801021         90    320011.3     1068   10570601  mmap          \n",
      "     0.0           586736         77      7619.9     2279      16757  open64        \n",
      "     0.0           108955          4     27238.8    25042      29002  pthread_create\n",
      "     0.0           101497         23      4412.9     1236      13988  fopen         \n",
      "     0.0            88438          3     29479.3    22367      43514  fgets         \n",
      "     0.0            76879         11      6989.0     4041      11807  write         \n",
      "     0.0            39635         14      2831.1     1152       4636  munmap        \n",
      "     0.0            31004          5      6200.8     2621       8899  open          \n",
      "     0.0            28176         16      1761.0     1020       3071  fclose        \n",
      "     0.0            23719         11      2156.3     1045       3324  read          \n",
      "     0.0            13319          3      4439.7     4205       4876  pipe2         \n",
      "     0.0            12227          2      6113.5     6010       6217  socket        \n",
      "     0.0             7388          2      3694.0     1408       5980  fgetc         \n",
      "     0.0             6354          4      1588.5     1370       1792  mprotect      \n",
      "     0.0             5784          2      2892.0     2331       3453  fread         \n",
      "     0.0             5757          1      5757.0     5757       5757  connect       \n",
      "     0.0             4195          2      2097.5     1036       3159  fcntl         \n",
      "     0.0             2453          1      2453.0     2453       2453  bind          \n",
      "     0.0             1534          1      1534.0     1534       1534  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report17.qdrep\"\n",
      "Report file moved to \"/dli/task/report17.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：在核函数中初始化向量\n",
    "\n",
    "当 `nsys profile` 给出核函数所需的执行时间时，则在此函数执行期间发生的主机到设备页错误和数据迁移都会包含在所显示的执行时间中。\n",
    "\n",
    "带着这样的想法来将 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 程序中的 `initWith` 主机函数重构为 CUDA 核函数，以便在 GPU 上并行初始化所分配的向量。成功编译及运行重构的应用程序后，但在对其进行分析之前，请假设如下内容：\n",
    "\n",
    "- 您期望重构会对 UM 页错误行为产生何种影响？\n",
    "- 您期望重构会对所报告的 `addVectorsInto` 运行时产生何种影响？\n",
    "\n",
    "请再次记录结果。如您遇到问题，请参阅 [解决方案](../edit/07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-6f99-93fb-a4ad-fb55.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-6f99-93fb-a4ad-fb55.qdrep\"\n",
      "Exporting 1764 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-6f99-93fb-a4ad-fb55.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  ----------  --------  ---------  ---------------------\n",
      "    78.2        208485625          3  69495208.3     19059  208421286  cudaMallocManaged    \n",
      "    13.8         36836747          1  36836747.0  36836747   36836747  cudaDeviceSynchronize\n",
      "     8.0         21290783          3   7096927.7   5967232    9233739  cudaFree             \n",
      "     0.0            52132          4     13033.0      5292      32315  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    94.8         34932812          3  11644270.7  11471099  11953655  initWith(float, float*, int)               \n",
      "     5.2          1913519          1   1913519.0   1913519   1913519  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "   100.0         21162880         768  27555.8     1599   179136  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    48.9        581624923         33  17624997.7    16893  100126883  poll          \n",
      "    42.4        504168441         32  15755263.8     9344  100064364  sem_timedwait \n",
      "     6.6         78886497        585    134848.7     1000   17739715  ioctl         \n",
      "     2.0         23621577         90    262462.0     1048    9181227  mmap          \n",
      "     0.0           580311         77      7536.5     2382      17810  open64        \n",
      "     0.0           130015          4     32503.8    28719      36774  pthread_create\n",
      "     0.0           108477         23      4716.4     1217      15438  fopen         \n",
      "     0.0            85882          3     28627.3    20681      43453  fgets         \n",
      "     0.0            82928         11      7538.9     4040      12348  write         \n",
      "     0.0            52184         14      3727.4     1245      16027  munmap        \n",
      "     0.0            32468          5      6493.6     2753       9326  open          \n",
      "     0.0            28185         16      1761.6     1013       3521  fclose        \n",
      "     0.0            23024         11      2093.1     1022       3036  read          \n",
      "     0.0            12836          3      4278.7     3611       4740  pipe2         \n",
      "     0.0            12199          2      6099.5     4427       7772  socket        \n",
      "     0.0             8691          3      2897.0     1685       4470  fread         \n",
      "     0.0             8281          2      4140.5     1610       6671  fgetc         \n",
      "     0.0             6381          4      1595.3     1462       1723  mprotect      \n",
      "     0.0             6026          1      6026.0     6026       6026  connect       \n",
      "     0.0             5474          2      2737.0     1138       4336  fcntl         \n",
      "     0.0             2104          1      2104.0     2104       2104  bind          \n",
      "     0.0             1355          1      1355.0     1355       1355  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report18.qdrep\"\n",
      "Report file moved to \"/dli/task/report18.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 异步内存预取\n",
    "\n",
    "在主机到设备和设备到主机的内存传输过程中，我们使用一种技术来减少页错误和按需内存迁移成本，此强大技术称为**异步内存预取**。通过此技术，程序员可以在应用程序代码使用统一内存 (UM) 之前，在后台将其异步迁移至系统中的任何 CPU 或 GPU 设备。此举可以减少页错误和按需数据迁移所带来的成本，并进而提高 GPU 核函数和 CPU 函数的性能。\n",
    "\n",
    "此外，预取往往会以更大的数据块来迁移数据，因此其迁移次数要低于按需迁移。此技术非常适用于以下情况：在运行时之前已知数据访问需求且数据访问并未采用稀疏模式。\n",
    "\n",
    "CUDA 可通过 `cudaMemPrefetchAsync` 函数，轻松将托管内存异步预取到 GPU 设备或 CPU。以下所示为如何使用该函数将数据预取到当前处于活动状态的 GPU 设备，然后再预取到 CPU：\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：异步内存预取\n",
    "\n",
    "此时，实验中的 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 程序不仅应启动 CUDA 核函数以将 2 个向量添加到第三个结果向量（所有向量均通过 `cudaMallocManaged` 函数进行分配），还应在 CUDA 核函数中并行初始化其中的每个向量。如果某种原因导致应用程序不执行上述任何操作，则请参阅以下 [参考应用程序](../edit/08-prefetch/01-vector-add-prefetch.cu)，并更新自己的代码库以反映其当前功能。\n",
    "\n",
    "在 [01-vector-add.cu](../edit/01-vector-add/01-vector-add.cu) 应用程序中使用 `cudaMemPrefetchAsync` 函数开展 3 个实验，以探究其会对页错误和内存迁移产生何种影响。\n",
    "\n",
    "- 当您将其中一个初始化向量预取到主机时会出现什么情况？\n",
    "- 当您将其中两个初始化向量预取到主机时会出现什么情况？\n",
    "- 当您将三个初始化向量全部预取到主机时会出现什么情况？\n",
    "\n",
    "在进行每个实验之前，请先假设 UM 的行为表现（尤其就页错误而言），以及其对所报告的初始化核函数运行时会产生何种影响，然后运行 `nsys profile` 进行验证。如您遇到问题，请参阅 [解决方案](../edit/08-prefetch/solutions/01-vector-add-prefetch-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：将内存预取回CPU\n",
    "\n",
    "请为该函数添加额外的内存预取回 CPU，以验证 `addVectorInto` 核函数的正确性。然后再次假设 UM 所受影响，并在 `nsys profile` 中进行分析确认。如您遇到问题，请参阅 [解决方案](../edit/08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "Success! All values calculated correctly.\n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-31fe-1054-8afc-fb40.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-31fe-1054-8afc-fb40.qdrep\"\n",
      "Exporting 8955 events: [==================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-31fe-1054-8afc-fb40.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls    Average     Minimum    Maximum           Name         \n",
      " -------  ---------------  ---------  -----------  ---------  ---------  ---------------------\n",
      "    42.8        214643325          3   71547775.0      18934  214578429  cudaMallocManaged    \n",
      "    30.5        153048078          1  153048078.0  153048078  153048078  cudaDeviceSynchronize\n",
      "    21.5        107725185          3   35908395.0   35030625   36881352  cudaMemPrefetchAsync \n",
      "     5.2         26194158          3    8731386.0    7795466   10509911  cudaFree             \n",
      "     0.0            69148          4      17287.0       4965      49296  cudaLaunchKernel     \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances   Average    Minimum   Maximum                      Name                    \n",
      " -------  ---------------  ---------  ----------  --------  --------  -------------------------------------------\n",
      "    98.8        151155769          3  50385256.3  50018828  50766308  initWith(float, float*, int)               \n",
      "     1.2          1901004          1   1901004.0   1901004   1901004  addVectorsInto(float*, float*, float*, int)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average  Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  -------  -------  -------  ---------------------------------\n",
      "    78.5         77265951        7144  10815.5     2175   172736  [CUDA Unified Memory memcpy HtoD]\n",
      "    21.5         21199968         768  27604.1     1599   179488  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total     Operations  Average  Minimum  Maximum               Operation            \n",
      " ----------  ----------  -------  -------  --------  ---------------------------------\n",
      " 393216.000        7144   55.041    4.000  1020.000  [CUDA Unified Memory memcpy HtoD]\n",
      " 131072.000         768  170.667    4.000  1020.000  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    49.5        956974973         53  18056131.6    19826  100126743  poll          \n",
      "    39.0        754107301         52  14502063.5    11325  100066170  sem_timedwait \n",
      "     9.9        191966500        592    324267.7     1004   36821071  ioctl         \n",
      "     1.5         28416744         90    315741.6     1115   10445367  mmap          \n",
      "     0.0           591185         77      7677.7     2187      16649  open64        \n",
      "     0.0           113588          4     28397.0    25667      34117  pthread_create\n",
      "     0.0           101961         23      4433.1     1139      15389  fopen         \n",
      "     0.0            86440          3     28813.3    19976      44301  fgets         \n",
      "     0.0            78366         11      7124.2     4010      10938  write         \n",
      "     0.0            39229         13      3017.6     1098       4859  munmap        \n",
      "     0.0            30656          5      6131.2     2843       8748  open          \n",
      "     0.0            26236         15      1749.1     1057       3027  fclose        \n",
      "     0.0            22709         10      2270.9     1223       3288  read          \n",
      "     0.0            12771          3      4257.0     3689       4586  pipe2         \n",
      "     0.0             8722          2      4361.0     3459       5263  socket        \n",
      "     0.0             8348          3      2782.7     1710       3884  fread         \n",
      "     0.0             7753          2      3876.5     1489       6264  fgetc         \n",
      "     0.0             6302          4      1575.5     1473       1697  mprotect      \n",
      "     0.0             6032          1      6032.0     6032       6032  connect       \n",
      "     0.0             5725          2      2862.5     1528       4197  fcntl         \n",
      "     0.0             1845          1      1845.0     1845       1845  bind          \n",
      "     0.0             1382          1      1382.0     1382       1382  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report21.qdrep\"\n",
      "Report file moved to \"/dli/task/report21.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用异步预取进行了一系列重构之后，您应该看到内存传输次数减少了，但是每次传输的量增加了，并且内核执行时间大大减少了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 总结\n",
    "\n",
    "此时，您在实验中能够执行以下操作：\n",
    "\n",
    "- 使用 **NVIDIA 命令行分析器** (**nsys**) 分析加速应用程序性能。\n",
    "- 利用对**流多处理器**的理解优化执行配置。\n",
    "- 理解**统一内存**在页错误和数据迁移方面的行为。\n",
    "- 使用**异步内存预取**减少页错误和数据迁移以提高性能。\n",
    "- 采用迭代开发周期快速加速和部署应用程序。\n",
    "\n",
    "\n",
    "为巩固您的学习成果，并加强您通过迭代方式加速、优化及部署应用程序的能力，请继续完成本实验的最后一个练习。完成后，时间富余并有意深究的学习者可以继续学习*高阶内容*部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 最后的练习：迭代优化加速的SAXPY应用程序\n",
    "\n",
    "[此处](../edit/09-saxpy/01-saxpy.cu) 为您提供一个基本的 [SAXPY](https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms#Level_1) 加速应用程序。该程序目前包含一些您需要找到并修复的错误，在此之后您才能使用 `nsys profile` 成功对其进行编译、运行和分析。\n",
    "\n",
    "在修复完错误并对应用程序进行分析后，您需记录 `saxpy` 核函数的运行时，然后采用*迭代方式*优化应用程序，并在每次迭代后使用 `nsys profile` 进行分析验证，以便了解代码更改对核函数性能和 UM 行为产生的影响。\n",
    "\n",
    "请充分运用本实验提供的各项技术。为获得更好的学习效果，请尽可能利用 [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) 提供的方法回想已经学过的内容，而不要急于在本课程开始之初就查阅技术细节。\n",
    "\n",
    "您的最终目标是在不修改 `N` 的情况下分析准确的 `saxpy` 核函数，以便在 *100us* 内运行。如您遇到问题，请参阅 [解决方案](../edit/09-saxpy/solutions/02-saxpy-solution.cu)，您亦可随时对其进行编译和分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \r\n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: LBR backtrace method is not supported on this platform. DWARF backtrace method will be used.\n",
      "Collecting data...\n",
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Processing events...\n",
      "Capturing symbol files...\n",
      "Saving temporary \"/tmp/nsys-report-448c-b001-a547-1ebd.qdstrm\" file to disk...\n",
      "Creating final output files...\n",
      "\n",
      "Processing [==============================================================100%]\n",
      "Saved report file to \"/tmp/nsys-report-448c-b001-a547-1ebd.qdrep\"\n",
      "Exporting 996 events: [===================================================100%]\n",
      "\n",
      "Exported successfully to\n",
      "/tmp/nsys-report-448c-b001-a547-1ebd.sqlite\n",
      "\n",
      "\n",
      "CUDA API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum           Name        \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------------\n",
      "    94.7        226455483          3  75485161.0    24836  226396714  cudaMallocManaged   \n",
      "     3.2          7706973          3   2568991.0   914665    5843018  cudaFree            \n",
      "     2.0          4806211          3   1602070.3   122777    3303918  cudaMemPrefetchAsync\n",
      "     0.0            48477          1     48477.0    48477      48477  cudaLaunchKernel    \n",
      "\n",
      "\n",
      "\n",
      "CUDA Kernel Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Instances  Average  Minimum  Maximum           Name          \n",
      " -------  ---------------  ---------  -------  -------  -------  -----------------------\n",
      "   100.0            34048          1  34048.0    34048    34048  saxpy(int*, int*, int*)\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by time):\n",
      "\n",
      " Time(%)  Total Time (ns)  Operations  Average   Minimum  Maximum              Operation            \n",
      " -------  ---------------  ----------  --------  -------  -------  ---------------------------------\n",
      "   100.0          8198656          24  341610.7   339840   350560  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (by size in KiB):\n",
      "\n",
      "   Total    Operations  Average   Minimum   Maximum               Operation            \n",
      " ---------  ----------  --------  --------  --------  ---------------------------------\n",
      " 49152.000          24  2048.000  2048.000  2048.000  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "\n",
      "\n",
      "Operating System Runtime API Statistics:\n",
      "\n",
      " Time(%)  Total Time (ns)  Num Calls   Average    Minimum   Maximum        Name     \n",
      " -------  ---------------  ---------  ----------  -------  ---------  --------------\n",
      "    45.1        364766052         19  19198213.3    11899  100061750  sem_timedwait \n",
      "    41.2        333534644         22  15160665.6    21130  100136659  poll          \n",
      "    12.7        102515614        589    174050.3     1016   17625448  ioctl         \n",
      "     0.7          5363105         90     59590.1     1116     938032  mmap          \n",
      "     0.2          1623111          2    811555.5    41057    1582054  sem_wait      \n",
      "     0.1           596872         77      7751.6     2313      17897  open64        \n",
      "     0.0           162980          5     32596.0    25753      51519  pthread_create\n",
      "     0.0           108539         12      9044.9     4034      26335  write         \n",
      "     0.0           105717         23      4596.4     1175      18226  fopen         \n",
      "     0.0            93086          3     31028.7    20849      43762  fgets         \n",
      "     0.0            31665         11      2878.6     1193       4549  munmap        \n",
      "     0.0            30245          5      6049.0     2544       8290  open          \n",
      "     0.0            27988         16      1749.3     1006       3680  fclose        \n",
      "     0.0            26127         11      2375.2     1002       3664  read          \n",
      "     0.0            12351          3      4117.0     3638       4547  pipe2         \n",
      "     0.0            10439          2      5219.5     3902       6537  socket        \n",
      "     0.0             8917          5      1783.4     1487       2385  mprotect      \n",
      "     0.0             7980          2      3990.0     1644       6336  fgetc         \n",
      "     0.0             7433          2      3716.5     3424       4009  fread         \n",
      "     0.0             5958          1      5958.0     5958       5958  connect       \n",
      "     0.0             5633          2      2816.5     1034       4599  fcntl         \n",
      "     0.0             2072          1      2072.0     2072       2072  bind          \n",
      "     0.0             1426          1      1426.0     1426       1426  listen        \n",
      "\n",
      "Report file moved to \"/dli/task/report23.qdrep\"\n",
      "Report file moved to \"/dli/task/report23.sqlite\"\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
