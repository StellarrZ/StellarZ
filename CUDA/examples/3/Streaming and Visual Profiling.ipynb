{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">异步流及 CUDA C/C++ 应用程序的可视化性能分析</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA工具包附带了 **Nsight Systems**，这是一个功能强大的GUI应用程序，可支持CUDA应用程序的开发。 Nsight Systems为被加速的应用程序生成图形化的活动时间表，其中包含有关CUDA API调用、内核执行、内存活动以及**CUDA流**的使用的详细信息。\n",
    "\n",
    "在本实验中，您将使用Nsight Systems时间表来指导您优化被加速的应用程序。此外，您还将学习一些中级的CUDA编程技术来支持您的工作：**手动的内存分配和迁移**； **固定**或**页面锁定**的主机内存；以及**非默认并发CUDA流**。\n",
    "\n",
    "本实验学习课程结束时，我们将为您提供一份评估测试，让您加速和优化一款简单的 n-body 模拟器，这可让您借此展示在本课程学习期间所掌握的技能。若您能够在保持正确性的同时加速模拟器，我们将为您颁发证书以资证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 预备知识\n",
    "\n",
    "为了充分利用本实验，您应该已经能够：\n",
    "\n",
    "- 编写，编译和运行既调用CPU函数又启动GPU核函数的C / C ++程序。\n",
    "- 使用执行配置(execution configuration)控制并行线程的层次结构。\n",
    "- 重构串行循环以在GPU上并行执行其迭代。\n",
    "- 分配和释放CUDA统一内存。\n",
    "- 了解统一内存在页面错误和数据迁移方面的行为。\n",
    "- 使用异步内存预取可以减少页面错误和数据迁移。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目标\n",
    "\n",
    "在完成本练习后，您将能够：\n",
    "\n",
    "- 使用**Nsight Systems**直观地描述由GPU加速的CUDA应用程序的时间表。\n",
    "- 使用**Nsight Systems**识别和利用CUDA应用程序中的优化机会。\n",
    "- 利用CUDA流在被加速的应用程序中并发执行核函数。\n",
    "- （ **可选的进阶内容** ）使用手动的设备内存分配，包括分配固定的内存，以便在并发CUDA流之间异步传输数据。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 运行Nsight Systems\n",
    "\n",
    "在此交互式实验环境中，我们设置了一个远程桌面，您可以从您的浏览器访问该桌面，并在其中启动和使用Nsight Systems。\n",
    "\n",
    "首先，为一个已经存在的向量加法程序创建一个性能报告文件，然后逐步执行一系列步骤，以在Nsight Systems中打开该报告文件，并提供良好的视觉体验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成报告文件\n",
    "\n",
    "[`01-vector-add.cu`](../edit/01-vector-add/01-vector-add.cu)（<--单击这个链接以在浏览器中对源文件进行编辑）包含一个正常的矢量加法程序。 使用下方的代码执行单元（通过按CTRL+ENTER，您可以执行它以及本实验中的任何代码执行单元）来编译并运行它。 您应该看到一条表明已成功完成的消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-no-prefetch 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，使用`nsys profile --stats = true`创建一个报告文件，您将可以在Nsight Systems可视化分析器中打开该文件。 在这里，我们使用-o标志为报告文件起一个容易记住的文件名："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-no-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-no-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-no-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t9108 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-no-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-no-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 9072 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-no-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   59.3       252336418           3      84112139.3           19017       252283247  cudaMallocManaged                                                               \n",
      "   35.1       149377977           1     149377977.0       149377977       149377977  cudaDeviceSynchronize                                                           \n",
      "    5.6        23935813           3       7978604.3         7251015         9136150  cudaFree                                                                        \n",
      "    0.0           52264           1         52264.0           52264           52264  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0       149366887           1     149366887.0       149366887       149366887  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   78.6        77542208        7251         10694.0            1824          128736  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   21.4        21056096         768         27416.8            1408          160000  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0            7251               54.2              4.000              764.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   53.2      1349903974          74      18241945.6           15401       100137203  poll                                                                            \n",
      "   42.0      1065469609          75      14206261.5            9486       100063182  sem_timedwait                                                                   \n",
      "    3.7        93991268         588        159849.1            1010        18621078  ioctl                                                                           \n",
      "    1.1        26696159          90        296624.0            1155         9077585  mmap                                                                            \n",
      "    0.0          596272          77          7743.8            2267           13439  open64                                                                          \n",
      "    0.0          112140           4         28035.0           22884           32111  pthread_create                                                                  \n",
      "    0.0           93191          23          4051.8            1422           14782  fopen                                                                           \n",
      "    0.0           86170           3         28723.3           19822           43661  fgets                                                                           \n",
      "    0.0           78552          11          7141.1            4093           10647  write                                                                           \n",
      "    0.0           39521          13          3040.1            1303            4839  munmap                                                                          \n",
      "    0.0           28120          16          1757.5            1090            2655  fclose                                                                          \n",
      "    0.0           25263           5          5052.6            2897            6995  open                                                                            \n",
      "    0.0           21102          10          2110.2            1105            3649  read                                                                            \n",
      "    0.0           10872           3          3624.0            3299            3919  pipe2                                                                           \n",
      "    0.0            8038           2          4019.0            3564            4474  socket                                                                          \n",
      "    0.0            6374           4          1593.5            1383            1917  mprotect                                                                        \n",
      "    0.0            5982           2          2991.0            2083            3899  fread                                                                           \n",
      "    0.0            4699           2          2349.5            1000            3699  fcntl                                                                           \n",
      "    0.0            4554           1          4554.0            4554            4554  connect                                                                         \n",
      "    0.0            1741           1          1741.0            1741            1741  bind                                                                            \n",
      "    0.0            1269           1          1269.0            1269            1269  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-no-prefetch-report ./vector-add-no-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开远程桌面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行下一个代码单元格以生成指向远程桌面的URL，您应该通过复制该URL并将其粘贴到新的浏览器选项卡中来打开该URL。 然后，按照notebook里的指示进行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = window.location.hostname + '/nsight/';\n",
       "element.append(url)\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = window.location.hostname + '/nsight/';\n",
    "element.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单击 _Connect_ 按钮后，远程桌面将要求您输入密码，即`nvidia`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开远程桌面终端应用程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，单击位于远程桌面屏幕底部的命令行终端应用程序图标："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![terminal](images/terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要打开Nsight Systems，请从现在打开的命令行终端中输入并运行`nsight-sys`（注意没有空格）命令："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open nsight](images/open-nsight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 启用使用情况报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出现提示时，单击“Yes”以启用使用情况报告："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![enable usage](images/enable_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打开报告文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过从Nsight Systems菜单访问 _File_-> _Open_ 来打开此报告文件，然后转到路径 `/root/Desktop/reports`并选择`vector-add-no-prefetch-report.qdrep`。 您在本实验中生成的所有报告都将位于此`root/Desktop/reports`目录中："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open-report](images/open-report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 忽略警告/错误信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以关闭并忽略看到的所有警告或错误，这仅仅是我们特定的远程桌面环境产生的结果："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ignore errors](images/ignore-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为活动时间表腾出更多的显示空间"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使您的体验更好，可全屏显示性能分析结果，即关闭 _Project Explorer_ 并隐藏 _Events View_ ："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![make nice](images/make-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "屏幕现在应如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![now nice](images/now-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 扩展CUDA统一内存的活动时间表"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，展开 _CUDA_-> _Unified memory_ 和 _Context_ 时间表，并关闭 _OS运行时库_ 的时间表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open memory](images/open-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 观察到有很多次的内存传输操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "略看一下即知，您的应用程序运行大约需要1秒钟，并且在运行`addVectorsInto`核函数期间，还有很多的UM内存活动："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memory and kernel](images/memory-and-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "放大内存的时间轴，可以更清楚地看到由按需内存页面错误引起的所有小内存传输。 几个提示：\n",
    "\n",
    "1. 您可以在滚动鼠标/触控板的同时按住`Ctrl`来放大和缩小时间轴的任何一点\n",
    "2. 您可以通过以下方式放大任意部分：单击+拖动它周围的矩形，然后选择 _Zoom in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是放大查看许多小的内存传输的示例："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![many transfers](images/many-transfers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 用Nsight Systems重复比较不同的代码重构后的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，您已经启动并运行了Nsight Systems，并且可以轻松地在活动时间表里移动位置。接下来，您将对您使用已经熟悉的技术进行了迭代改进的程序进行性能分析。 每做一次性能分析，活动时间表都会对下一步如何修改代码提供有用的信息。 这样做还将使您更深地理解各种CUDA编程技术如何影响应用程序的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：比较预取与不预取的活动时间表\n",
    "\n",
    "[`01-vector-add-prefetch-solution.cu`](../edit/01-vector-add/solutions/01-vector-add-prefetch-solution.cu)对上面的向量加法程序进行重构，以便它的`addVectorsInto`核函数所需的3个向量在核函数被调用之前就被异步预取到活动的GPU设备上（使用[`cudaMemPrefetchAsync`](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42)）。 请打开源代码，并确定这些更改在应用程序中的何处进行的。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印出来的成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-prefetch 01-vector-add/solutions/01-vector-add-prefetch-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t2048 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 2012 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   69.7       215494461           3      71831487.0           17712       215437974  cudaMallocManaged                                                               \n",
      "   19.0        58887541           1      58887541.0        58887541        58887541  cudaDeviceSynchronize                                                           \n",
      "    7.6        23578256           3       7859418.7         6736505         9136695  cudaFree                                                                        \n",
      "    3.7        11300253           3       3766751.0            7979        11170399  cudaMemPrefetchAsync                                                            \n",
      "    0.0           44400           1         44400.0           44400           44400  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0         1701587           1       1701587.0         1701587         1701587  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   75.6        65699040         192        342182.5          339904          344832  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   24.4        21169792         768         27564.8            1664          159840  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0             192             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   51.8      1184361572          69      17164660.5           21809       100135417  poll                                                                            \n",
      "   40.6       929437375          66      14082384.5           10258       100062604  sem_timedwait                                                                   \n",
      "    5.9       134055045         589        227597.7            1017        22822731  ioctl                                                                           \n",
      "    1.2        26355893          90        292843.3            1141         9080525  mmap                                                                            \n",
      "    0.5        11648341           2       5824170.5           38210        11610131  sem_wait                                                                        \n",
      "    0.0          564483          77          7330.9            2308           12468  open64                                                                          \n",
      "    0.0          157382           5         31476.4           23711           47850  pthread_create                                                                  \n",
      "    0.0           92858          23          4037.3            1257           13264  fopen                                                                           \n",
      "    0.0           91731          12          7644.2            4322           10404  write                                                                           \n",
      "    0.0           88361           3         29453.7           22412           43521  fgets                                                                           \n",
      "    0.0           39925          14          2851.8            1212            4304  munmap                                                                          \n",
      "    0.0           27891          16          1743.2            1095            2980  fclose                                                                          \n",
      "    0.0           26204           5          5240.8            2812            7066  open                                                                            \n",
      "    0.0           25954          12          2162.8            1018            3280  read                                                                            \n",
      "    0.0           11378           3          3792.7            3031            4751  pipe2                                                                           \n",
      "    0.0            8881           5          1776.2            1560            2154  mprotect                                                                        \n",
      "    0.0            6921           2          3460.5            3197            3724  socket                                                                          \n",
      "    0.0            5737           2          2868.5            2010            3727  fread                                                                           \n",
      "    0.0            5324           3          1774.7            1026            3267  fcntl                                                                           \n",
      "    0.0            5190           1          5190.0            5190            5190  connect                                                                         \n",
      "    0.0            1976           1          1976.0            1976            1976  bind                                                                            \n",
      "    0.0            1173           1          1173.0            1173            1173  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-prefetch-report ./vector-add-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开报告，让先前的报告保持打开的状态以进行比较。\n",
    "\n",
    "- 在添加异步预取之前，执行时间与`addVectorsInto`核函数的执行时间相比如何？\n",
    "- 在活动时间表的 *CUDA API* 部分中找到`cudaMemPrefetchAsync`。\n",
    "- 内存传输有什么变化？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：使用核函数进行向量初始化并分析其性能\n",
    "\n",
    "在向量加法程序的先前迭代中，向量数据是在CPU上初始化的，因此需要在addVectorsInto核函数对其进行操作之前将其迁移到GPU上。\n",
    "\n",
    "该应用程序的下一个迭代[01-init-kernel-solution.cu](../edit/02-init-kernel/solutions/01-init-kernel-solution.cu)已重构为在GPU上并行处理初始化数据。\n",
    "\n",
    "由于初始化现是在GPU上进行的，因此预取要在初始化之前完成的，而不是在向量加法之前完成的。 查看源代码以识别在何处进行了这些更改。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-kernel 02-init-kernel/solutions/01-init-kernel-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-kernel-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-kernel\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-kernel-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1780 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-kernel-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-kernel-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1749 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-kernel-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   89.7       216874590           3      72291530.0           18826       216817986  cudaMallocManaged                                                               \n",
      "    7.6        18271543           3       6090514.3          829754        16521607  cudaFree                                                                        \n",
      "    2.4         5775979           1       5775979.0         5775979         5775979  cudaDeviceSynchronize                                                           \n",
      "    0.4          880903           3        293634.3            8562          793105  cudaMemPrefetchAsync                                                            \n",
      "    0.0           70485           4         17621.3            8934           38093  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.2         1865490           3        621830.0          619195          623868  initWith                                                                        \n",
      "   47.8         1709972           1       1709972.0         1709972         1709972  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        21154688         768         27545.2            1632          160064  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   45.1       462159396          29      15936530.9           12383       100062529  sem_timedwait                                                                   \n",
      "   44.1       451609642          31      14568053.0           20897       100117109  poll                                                                            \n",
      "    8.7        88959839         585        152068.1            1002        18627543  ioctl                                                                           \n",
      "    2.1        21098725          91        231854.1            1157        16471808  mmap                                                                            \n",
      "    0.1          599196          77          7781.8            2327           13937  open64                                                                          \n",
      "    0.0          145787           5         29157.4           22405           37538  pthread_create                                                                  \n",
      "    0.0           89752          12          7479.3            4232           11673  write                                                                           \n",
      "    0.0           89411          23          3887.4            1137           12464  fopen                                                                           \n",
      "    0.0           87974           3         29324.7           21401           43405  fgets                                                                           \n",
      "    0.0           85527           2         42763.5           24107           61420  sem_wait                                                                        \n",
      "    0.0           38845          15          2589.7            1505            3958  munmap                                                                          \n",
      "    0.0           28752          16          1797.0            1097            3748  fclose                                                                          \n",
      "    0.0           27009          12          2250.8            1029            3928  read                                                                            \n",
      "    0.0           24460           5          4892.0            2739            6592  open                                                                            \n",
      "    0.0           11533           3          3844.3            3569            4076  pipe2                                                                           \n",
      "    0.0            7507           3          2502.3            1583            3959  fread                                                                           \n",
      "    0.0            7476           5          1495.2            1436            1548  mprotect                                                                        \n",
      "    0.0            7004           2          3502.0            3359            3645  socket                                                                          \n",
      "    0.0            4794           1          4794.0            4794            4794  connect                                                                         \n",
      "    0.0            4582           2          2291.0            1016            3566  fcntl                                                                           \n",
      "    0.0            1771           1          1771.0            1771            1771  bind                                                                            \n",
      "    0.0            1236           1          1236.0            1236            1236  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-kernel-report ./init-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开新的报告文件，然后执行以下操作：\n",
    "\n",
    "- 将应用程序和`addVectorsInto`运行时间与应用程序的先前版本进行比较，有什么变化？\n",
    "- 查看时间表中的 *Kernels* 部分。 这两个核函数（`addVectorsInto`和初始化核函数）中的哪一个占用了GPU的大部分时间？\n",
    "- 您的应用程序包含以下哪项？\n",
    "   - 数据迁移（HtoD）\n",
    "   - 数据迁移（DtoH）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：使用异步预将数据取回主机，并分析其性能\n",
    "\n",
    "目前，向量加法应用程序是在主机上验证向量加法核函数的结果，所以在该应用程序的下一个重构[01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu)中，我们用异步预取将数据取回主机后再进行验证。\n",
    "\n",
    "在检查了更改之后，使用下方的代码执行单元来编译并运行重构的应用程序。 您应该看到打印出来的成功消息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-host 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在为该版本的应用程序创建一个报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/prefetch-to-host-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./prefetch-to-host\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/prefetch-to-host-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1073 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/prefetch-to-host-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/prefetch-to-host-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1042 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/prefetch-to-host-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   84.5       215959331           3      71986443.7           18552       215898684  cudaMallocManaged                                                               \n",
      "    8.6        21969048           4       5492262.0            8787        21075459  cudaMemPrefetchAsync                                                            \n",
      "    4.6        11827293           3       3942431.0          820962        10090534  cudaFree                                                                        \n",
      "    2.3         5785231           1       5785231.0         5785231         5785231  cudaDeviceSynchronize                                                           \n",
      "    0.0           68934           4         17233.5            9546           36700  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.1         1863059           3        621019.7          616284          626588  initWith                                                                        \n",
      "   47.9         1711060           1       1711060.0         1711060         1711060  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20351712          64        317995.5          311040          319840  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.5       421551243          28      15055401.5           20693       100114851  poll                                                                            \n",
      "   43.5       421462657          25      16858506.3           10430       100064035  sem_timedwait                                                                   \n",
      "   11.3       109595415         587        186704.3            1013        21041355  ioctl                                                                           \n",
      "    1.5        14564992          91        160054.9            1213        10013108  mmap                                                                            \n",
      "    0.1          566826          77          7361.4            2280           12391  open64                                                                          \n",
      "    0.0          144531           5         28906.2           22762           37729  pthread_create                                                                  \n",
      "    0.0          106634          23          4636.3            1216           15554  fopen                                                                           \n",
      "    0.0           89287          12          7440.6            3888           11915  write                                                                           \n",
      "    0.0           86060           3         28686.7           20440           43519  fgets                                                                           \n",
      "    0.0           85648           2         42824.0           24817           60831  sem_wait                                                                        \n",
      "    0.0           69778          16          4361.1            1239           32098  munmap                                                                          \n",
      "    0.0           27518          16          1719.9            1082            2641  fclose                                                                          \n",
      "    0.0           26174           5          5234.8            2812            7469  open                                                                            \n",
      "    0.0           25452          13          1957.8            1032            2872  read                                                                            \n",
      "    0.0           11603           3          3867.7            3435            4452  pipe2                                                                           \n",
      "    0.0            7952           2          3976.0            3667            4285  socket                                                                          \n",
      "    0.0            7836           5          1567.2            1406            1812  mprotect                                                                        \n",
      "    0.0            7330           3          2443.3            1483            3933  fread                                                                           \n",
      "    0.0            4336           1          4336.0            4336            4336  connect                                                                         \n",
      "    0.0            3409           1          3409.0            3409            3409  fcntl                                                                           \n",
      "    0.0            1683           1          1683.0            1683            1683  bind                                                                            \n",
      "    0.0            1218           1          1218.0            1218            1218  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o prefetch-to-host-report ./prefetch-to-host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开此报告文件，然后执行以下操作：\n",
    "\n",
    "- 使用时间表里的 *Unified Memory*部分，对比向CPU添加预取之前和之后的*Data Migration (DtoH)*事件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 并发CUDA流\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您现在将学习一个新概念 **CUDA Streams**。 在对它们进行介绍之后，您将返回使用Nsight Systems来更好地评估它们对应用程序性能的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下幻灯片以可视化的方式概要地介绍了即将学习的内容。请在进入以下更详细的内容之前浏览每一页幻灯片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-1-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 CUDA 编程中，**流**是由按顺序执行的一系列命令构成。在 CUDA 应用程序中，核函数的执行以及一些内存传输均在 CUDA 流中进行。不过直至此时，您仍未直接与 CUDA 流打交道；但实际上您的 CUDA 代码已在名为*默认流*的流中执行了其核函数。\n",
    "\n",
    "除默认流以外，CUDA 程序员还可创建并使用非默认 CUDA 流，此举可支持执行多个操作，例如在不同的流中并发执行多个核函数。多流的使用可以为您的加速应用程序带来另外一个层次的并行，并能提供更多应用程序的优化机会。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 控制CUDA流行为的规则\n",
    "\n",
    "为有效利用 CUDA 流，您应了解有关 CUDA 流行为的以下几项规则：\n",
    "\n",
    "- 给定流中的所有操作会按序执行。\n",
    "- 就不同非默认流中的操作而言，无法保证其会按彼此之间的任何特定顺序执行。\n",
    "- 默认流具有阻断能力，即，它会等待其它已在运行的所有流完成当前操作之后才运行，但在其自身运行完毕之前亦会阻碍其它流的运行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建，使用和销毁非默认CUDA流\n",
    "\n",
    "以下代码段演示了如何创建，利用和销毁非默认CUDA流。您会注意到，要在非默认CUDA流中启动CUDA核函数，必须将流作为执行配置的第4个可选参数传递给该核函数。到目前为止，您仅利用了执行配置的前两个参数：\n",
    "\n",
    "``` C++\n",
    "cudaStream_t stream;   // CUDA流的类型为 `cudaStream_t`\n",
    "cudaStreamCreate(&stream); // 注意，必须将一个指针传递给 `cudaCreateStream`\n",
    "\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>();   // `stream` 作为第4个EC参数传递\n",
    "\n",
    "cudaStreamDestroy(stream); // 注意，将值（而不是指针）传递给 `cudaDestroyStream`\n",
    "```\n",
    "\n",
    "但值得一提的是，执行配置的第3个可选参数超出了本实验的范围。此参数允许程序员提供**共享内存**（当前将不涉及的高级主题）中为每个内核启动动态分配的字节数。每个块分配给共享内存的默认字节数为“0”，在本练习的其余部分中，您将传递“ 0”作为该值，以便展示我们感兴趣的第4个参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：预测默认流行为\n",
    "\n",
    "[01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) 应用程序带有一个非常简单的 `printNumber` 核函数，可用于接受及打印整数。仅在单个线程块内使用单线程执行该核函数，但使用“for 循环”可执行 5 次，并且每次启动时都会传递“for 循环”的迭代次数。\n",
    "\n",
    "使用下方的代码执行线程块编译和运行 [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) 应用程序。您应能看到打印的数字 `0` 至 `4`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既已了解核函数在默认情况下会在默认流中执行，那么据您预计，`print-numbers` 程序的 5 次启动将会顺次执行还是会并行执行？您应能提及默认流的两个功能来支持您的回答。在下面的单元格中创建一个报告文件，然后在Nsight Systems中将其打开以确认您的答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./print-numbers\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t941 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 911 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       205219611           5      41043922.2            5003       205194359  cudaLaunchKernel                                                                \n",
      "    0.1          270528           1        270528.0          270528          270528  cudaDeviceSynchronize                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          283742           5         56748.4           54912           63391  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.0       238204072          14      17014576.6           10343       100059643  sem_timedwait                                                                   \n",
      "   37.8       204483584          14      14605970.3           21542       100120708  poll                                                                            \n",
      "   17.5        94613008         573        165118.7            1006        18595029  ioctl                                                                           \n",
      "    0.6         3091317          82         37699.0            1168         1127164  mmap                                                                            \n",
      "    0.1          582122          77          7560.0            2245           11939  open64                                                                          \n",
      "    0.0          102728           4         25682.0           21469           30470  pthread_create                                                                  \n",
      "    0.0           99259          23          4315.6            1326           12559  fopen                                                                           \n",
      "    0.0           96759          12          8063.2            3749           14935  write                                                                           \n",
      "    0.0           89765           3         29921.7           22299           44235  fgets                                                                           \n",
      "    0.0           28340          16          1771.3            1101            2947  fclose                                                                          \n",
      "    0.0           26999          13          2076.8            1009            2979  read                                                                            \n",
      "    0.0           24309           5          4861.8            2978            7153  open                                                                            \n",
      "    0.0           21705           8          2713.1            1260            3747  munmap                                                                          \n",
      "    0.0           11179           3          3726.3            3147            4240  pipe2                                                                           \n",
      "    0.0            7629           2          3814.5            3553            4076  socket                                                                          \n",
      "    0.0            5858           2          2929.0            2011            3847  fread                                                                           \n",
      "    0.0            5770           4          1442.5            1310            1640  mprotect                                                                        \n",
      "    0.0            4586           1          4586.0            4586            4586  connect                                                                         \n",
      "    0.0            4496           2          2248.0            1096            3400  fcntl                                                                           \n",
      "    0.0            2460           1          2460.0            2460            2460  fflush                                                                          \n",
      "    0.0            1801           1          1801.0            1801            1801  bind                                                                            \n",
      "    0.0            1227           1          1227.0            1227            1227  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-report ./print-numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：实现并发CUDA流\n",
    "\n",
    "由于核函数的所有 5 次启动均在同一个流中发生，因此看到 5 个核函数顺次执行也就不足为奇。此外，也可以这么说，由于默认流具有阻断作用，所以核函数都会在完成本次启动之后才启动下一次，而事实也是如此。\n",
    "\n",
    "请重构 [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) 应用程序，以便核函数的每次启动都在自身非默认流中进行。若已不再需要所创建的流，请务必予以销毁。请使用下方的代码执行单元编译和运行经重构的代码。您应仍能看到打印的数字 `0` 至 `4`，不过这些数字不一定会按升序排列。如您遇到问题，请参阅 [解决方案](../edit/05-stream-intro/solutions/01-print-numbers-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers-in-streams 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您既已为核函数的 5 次启动使用 5 个不同的非默认流，您预计它们会顺次执行还是会并行执行？除目前对流的了解之外，您还需考虑 `printNumber` 核函数的简单程度，也就是说，即使您预测并行运行，核函数的完成速度是否仍会允许完全重叠？\n",
    "\n",
    "进行假设后，在Nsight Systems中打开一个新的报告文件以查看其实际行为。 您应该注意到，现在，_CUDA_ 部分中为您创建的每个非默认流提供了其他行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-in-streams-report2\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = print-numbers-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-in-streams-report2.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t958 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-in-streams-report2.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-in-streams-report2.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 923 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-in-streams-report2.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       204201336           5      40840267.2            2014       204192086  cudaStreamCreate                                                                \n",
      "    0.0           82031           5         16406.2            7360           49548  cudaLaunchKernel                                                                \n",
      "    0.0           66950           1         66950.0           66950           66950  cudaDeviceSynchronize                                                           \n",
      "    0.0           20870           5          4174.0            3437            6273  cudaStreamDestroy                                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          303550           5         60710.0           56031           65824  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.7       234564483          14      16754605.9           10471       100060423  sem_timedwait                                                                   \n",
      "   37.9       203492793          14      14535199.5           18267       100126122  poll                                                                            \n",
      "   17.5        94120467         574        163972.9            1046        18893207  ioctl                                                                           \n",
      "    0.6         3118500          82         38030.5            1154         1103819  mmap                                                                            \n",
      "    0.1          564943          77          7336.9            2299           11551  open64                                                                          \n",
      "    0.0          104440           4         26110.0           22576           29236  pthread_create                                                                  \n",
      "    0.0           91985          12          7665.4            4330           10348  write                                                                           \n",
      "    0.0           90022          23          3914.0            1184           13112  fopen                                                                           \n",
      "    0.0           86109           3         28703.0           19947           43517  fgets                                                                           \n",
      "    0.0           28016          16          1751.0            1075            3022  fclose                                                                          \n",
      "    0.0           27333          14          1952.4            1022            2720  read                                                                            \n",
      "    0.0           24541           5          4908.2            3034            7128  open                                                                            \n",
      "    0.0           24156           9          2684.0            1036            4057  munmap                                                                          \n",
      "    0.0           11441           3          3813.7            3337            4386  pipe2                                                                           \n",
      "    0.0            8189           2          4094.5            3700            4489  socket                                                                          \n",
      "    0.0            6093           4          1523.3            1345            1746  mprotect                                                                        \n",
      "    0.0            5318           2          2659.0            2014            3304  fread                                                                           \n",
      "    0.0            4967           1          4967.0            4967            4967  connect                                                                         \n",
      "    0.0            3350           1          3350.0            3350            3350  fcntl                                                                           \n",
      "    0.0            2893           1          2893.0            2893            2893  fflush                                                                          \n",
      "    0.0            1697           1          1697.0            1697            1697  bind                                                                            \n",
      "    0.0            1223           1          1223.0            1223            1223  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-in-streams-report2 print-numbers-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![stream print](images/streams-print.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：将流用于并行进行数据初始化的核函数\n",
    "\n",
    "您一直使用的向量加法应用程序 [01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu) 目前启动 3 次初始化核函数，即：为 `vectorAdd` 核函数需要初始化的 3 个向量分别启动一次。重构该应用程序，以便在其各自的非默认流中启动全部 3 个初始化核函数。在使用下方的代码执行单元编译及运行时，您应仍能看到打印的成功消息。如您遇到问题，请参阅 [解决方案](../edit/06-stream-init/solutions/01-stream-init-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-in-streams 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在Nsight Systems中打开一个报告，以确认您的3个初始化内核启动正在它们自己的非默认流中运行，并且存在一定程度的并发重叠。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1075 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1044 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   84.5       215770821           3      71923607.0           18836       215708613  cudaMallocManaged                                                               \n",
      "    8.6        21956401           4       5489100.2            7206        21074929  cudaMemPrefetchAsync                                                            \n",
      "    4.6        11849135           3       3949711.7          828165        10111924  cudaFree                                                                        \n",
      "    2.3         5781089           1       5781089.0         5781089         5781089  cudaDeviceSynchronize                                                           \n",
      "    0.0           58690           4         14672.5            9133           29590  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   52.2         1866771           3        622257.0          617948          625692  initWith                                                                        \n",
      "   47.8         1711667           1       1711667.0         1711667         1711667  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        20339520          64        317805.0          311072          319840  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   43.5       421576185          25      16863047.4           12041       100059744  sem_timedwait                                                                   \n",
      "   43.5       421244909          28      15044461.0           17982       100113665  poll                                                                            \n",
      "   11.3       109697255         590        185927.6            1009        21040007  ioctl                                                                           \n",
      "    1.5        14664930          91        161153.1            1018        10024393  mmap                                                                            \n",
      "    0.1          556290          77          7224.5            3118           11809  open64                                                                          \n",
      "    0.0          143418           5         28683.6           22244           36427  pthread_create                                                                  \n",
      "    0.0           92267          12          7688.9            4427           10306  write                                                                           \n",
      "    0.0           91641          23          3984.4            1408           12000  fopen                                                                           \n",
      "    0.0           90020           2         45010.0           28942           61078  sem_wait                                                                        \n",
      "    0.0           87496           3         29165.3           20591           44845  fgets                                                                           \n",
      "    0.0           72392          14          5170.9            1279           39042  munmap                                                                          \n",
      "    0.0           27987          16          1749.2            1099            3060  fclose                                                                          \n",
      "    0.0           27162          14          1940.1            1013            3435  read                                                                            \n",
      "    0.0           24544           5          4908.8            3145            6648  open                                                                            \n",
      "    0.0           11099           3          3699.7            3265            4125  pipe2                                                                           \n",
      "    0.0            7805           5          1561.0            1471            1634  mprotect                                                                        \n",
      "    0.0            7700           3          2566.7            1541            4200  fread                                                                           \n",
      "    0.0            7359           2          3679.5            3420            3939  socket                                                                          \n",
      "    0.0            4836           1          4836.0            4836            4836  connect                                                                         \n",
      "    0.0            3174           1          3174.0            3174            3174  fcntl                                                                           \n",
      "    0.0            1793           1          1793.0            1793            1793  bind                                                                            \n",
      "    0.0            1497           1          1497.0            1497            1497  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-in-streams-report ./init-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 总结\n",
    "\n",
    "在本实验中，您学习了：\n",
    "\n",
    "- 使用 **Nsight Systems** 直观地描述GPU加速的CUDA应用程序的活动时间表。\n",
    "- 使用Nsight Systems识别和利用GPU加速的CUDA应用程序中的优化机会。\n",
    "- 利用CUDA流在被加速的应用程序中并发执行核函数。\n",
    "\n",
    "此时，您拥有大量的基本工具和技术，可用于加速原本只能在CPU上的应用程序，然后不断优化那些被加速的应用程序。 在最后的练习中，您将有机会应用学到的所有知识来加速[n-body](https://en.wikipedia.org/wiki/N-body_problem)模拟器，以预测受引力互相影响的一组物体的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 最后的练习：加速和优化N体模拟器\n",
    "\n",
    "[n-body](https://en.wikipedia.org/wiki/N-body_problem) 模拟器可以预测通过引力相互作用的一组物体的个体运动。[01-nbody.cu](../edit/09-nbody/01-nbody.cu) 包含一个简单而有效的 n-body 模拟器，适合用于在三维空间移动的物体。我们可通过向该应用程序传递一个命令行参数以影响系统中的物体数量。\n",
    "\n",
    "以目前的仅用CPU的情况下，此应用程序大约需要5秒钟才能运行4096个物体，需要**20分钟**才能运行65536个物体。您的任务是用GPU加速程序，同时保持仿真的正确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 做此练习时可考虑的注意事项\n",
    "\n",
    "在开始任务之前，请注意以下事项：\n",
    "\n",
    "- 在第一次重构中，请格外注意应用程序的逻辑部分（尤其是 `bodyForce` 函数）能够并且应该基本保持不变：要侧重于尽可能轻松地加速应用程序。\n",
    "- 代码库包含 `main` 函数内的“for 循环”，用于将 `bodyForce` 函数计算的物体间的引力集成到系统中每个物体的所在位置。该集成不仅需在 `bodyForce` 函数运行后进行，并且需在下一次调用 `bodyForce` 函数之前完成。在选择并行化的处理方式和程序位置时，请牢记这一点。\n",
    "- 使用基于性能分析驱动和迭代的方法。\n",
    "- 无需为代码添加错误处理，但其可能有助您确保代码的顺利运行。\n",
    "\n",
    "快去开心地执行任务吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此单元格来编译nbody模拟器。尽管它最初只是一个仅用于CPU的应用程序，但却可以准确地模拟物体的位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -std=c++11 -o nbody 09-nbody/01-nbody.cu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强烈建议您使用性能分析器来协助您的工作。执行以下单元格以生成性能分析报告文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/nbody-report\n",
      "\tforce-overwrite = true\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./nbody\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "0.041 Billion Interactions / secondThe application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u0000\tGenerating the /dli/task/nbody-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t55 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/nbody-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/nbody-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 34 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/nbody-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CUDA trace data was not collected.\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0           15791           2          7895.5            6953            8838  fopen64                                                                         \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true --force-overwrite=true -o nbody-report ./nbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们导入一个函数，该函数将针对各种数量的物体运行您的`nbody`模拟器，以检查其性能和准确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessment import run_assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行以下单元格以运行并评估`nbody`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用4096个物体运行n-体模拟器\n",
      "----------------------------------------\n",
      "\n",
      "此应用程序应该运行得快于0.9秒。\n",
      "您的应用程序运行了: 4.0772秒\n",
      "您的应用程序仍不够快。\n"
     ]
    }
   ],
   "source": [
    "run_assessment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成证书"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果您通过了评估，请返回课程页面（如下所示）并单击\"ASSESS TASK\"（评估任务）按钮，这将为您生成该课程的证书。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run_assessment](./images/run_assessment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进阶内容\n",
    "\n",
    "以下章节专为时间富余和有意深究的学习者而设，其中将介绍更多中级技术，其中会涉及部分手动内存管理，以及使用非默认流重叠执行核函数和内存拷贝。\n",
    "\n",
    "在了解以下所列的各项技术后，您可尝试运用这些技术进一步优化 n-body 模拟器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 手动内存分配和复制\n",
    "\n",
    "尽管 `cudaMallocManaged` 和 `cudaMemPrefetchAsync` 函数性能出众并能大幅简化内存迁移，但有时也有必要使用更多手动内存分配方法。这在已知只需在设备或主机上访问数据时尤其如此，并且因免于进行自动按需迁移而能够收回数据迁移成本。\n",
    "\n",
    "此外，通过手动内存管理，您可以使用非默认流同时开展数据传输与计算工作。在本节中，您将学习一些基本的手动内存分配和拷贝技术，之后会延伸应用这些技术以同时开展数据拷贝与计算工作。\n",
    "\n",
    "以下是一些用于手动内存管理的 CUDA 命令：\n",
    "\n",
    "- `cudaMalloc` 命令将直接为处于活动状态的 GPU 分配内存。这可防止出现所有 GPU 分页错误，而代价是主机代码将无法访问该命令返回的指针。\n",
    "- `cudaMallocHost` 命令将直接为 CPU 分配内存。该命令可“固定”内存(pinned memory)或“页锁定”内存(page-locked memory)，此举允许将内存异步拷贝至 GPU 或从 GPU 异步拷贝至内存。固定内存过多则会干扰 CPU 性能，因此请勿无端使用该命令。释放固定内存时应使用 `cudaFreeHost` 命令。\n",
    "- 无论是从主机到设备还是从设备到主机，`cudaMemcpy` 命令均可拷贝（而非传输）内存。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手动内存管理示例\n",
    "\n",
    "以下是一段演示使用上述 CUDA API 调用的代码。\n",
    "\n",
    "```cpp\n",
    "int *host_a, *device_a;        // Define host-specific and device-specific arrays.\n",
    "cudaMalloc(&device_a, size);   // `device_a` is immediately available on the GPU.\n",
    "cudaMallocHost(&host_a, size); // `host_a` is immediately available on CPU, and is page-locked, or pinned.\n",
    "\n",
    "initializeOnHost(host_a, N);   // No CPU page faulting since memory is already allocated on the host.\n",
    "\n",
    "// `cudaMemcpy` takes the destination, source, size, and a CUDA-provided variable for the direction of the copy.\n",
    "cudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "kernel<<<blocks, threads, 0, someStream>>>(device_a, N);\n",
    "\n",
    "// `cudaMemcpy` can also copy data from device to host.\n",
    "cudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "verifyOnHost(host_a, N);\n",
    "\n",
    "cudaFree(device_a);\n",
    "cudaFreeHost(host_a);          // Free pinned memory like this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：手动分配主机和设备内存\n",
    "\n",
    "向量加法应用程序[01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu)的最新迭代使用 `cudaMallocManaged` 命令分配受管理的内存，该内存首先由初始化核函数在设备上使用，然后由向量加法核函数在设备上使用，然后再由主机所使用以对加法结果做验证。此时内存数据的传输是自动进行的。这是种很明智的方法，但我们也值得尝试一些手动内存分配和拷贝方法，以观察其对应用程序性能的影响。\n",
    "\n",
    "将 [01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu) 应用程序重构为**不**使用 `cudaMallocManaged` 命令。为此，您需要执行以下操作：\n",
    "\n",
    "- 将调用 `cudaMallocManaged` 命令替换为调用 `cudaMalloc` 命令。\n",
    "- 创建将用于在主机上验证的额外向量。这是由于使用 `cudaMalloc` 命令分配的内存在主机上不可用，因此您必须执行此操作, 使用 `cudaMallocHost` 命令分配此主机向量。\n",
    "- 在 `addVectorsInto` 核函数运行完毕后，使用 `cudaMemcpy` 命令将包含相加结果的向量复制到使用 `cudaMallocHost` 命令创建的主机向量中。\n",
    "- 使用 `cudaFreeHost` 命令释放经由 `cudaMallocHost` 命令分配的内存。\n",
    "\n",
    "如您遇到问题，请参阅 [解决方案](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector-add-manual-alloc 06-stream-init/solutions/01-stream-init-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成重构后，在Nsight Systems中打开一个报告，并使用活动时间表执行以下操作：\n",
    "\n",
    "- 注意，时间表的*统一内存*部分将不复存在。\n",
    "- 比较此时间表与之前重构的时间表，并使用时间轴标尺比较当前应用程序中 `cudaMalloc` 的运行时间与先前应用程序中 `cudaMallocManaged` 的运行时间。\n",
    "- 查看当前应用程序中初始化核函数的运行开始时间如何会晚于其在上次迭代中的运行时间。检查过时间轴后，您将发现时间差在于 `cudaMallocHost` 命令所用的时间。这很清楚地表明内存传输与内存拷贝的区别。正如您当前的操作，拷贝内存时，数据将存在于系统中的 2 个不同位置。与在上次迭代中仅分配 3 个向量相比，当前分配第 4 个主机向量会产生较小的性能成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 使用流实现数据传输和代码的重叠执行\n",
    "\n",
    "以下幻灯片概要地讲解了后面将涉及的内容。请点击浏览一遍这些幻灯片，然后再继续深入了解以下章节中的主题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-3-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-01-V1/AC_STREAMS_NVVP-zh/NVVP-Streams-3-zh.pptx\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了`cudaMemcpy`之外，还有`cudaMemcpyAsync`，只要固定了主机内存，它就可以从主机到设备或从设备到主机异步复制内存，这可以通过使用`cudaMallocHost`分配它来完成。\n",
    "\n",
    "与核函数的执行类似，`cudaMemcpyAsync`在默认情况下仅相对于主机是异步的。默认情况下，它在默认流中执行，因此对于GPU上发生的其他CUDA操作而言，它是阻塞操作。但是，`cudaMemcpyAsync`函数将非默认流作为可选的第5个参数。通过向其传递非默认流，可以将内存传输与其他非默认流中发生的其他CUDA操作并发。\n",
    "\n",
    "一种常见且有用的模式是结合使用固定主机内存，非默认流中的异步内存副本和非默认流中的核函数执行，以使内存传输与核函数的执行重叠。\n",
    "\n",
    "在以下示例中，我们并非在等待整个内存拷贝完成之后再开始运行核函数，而是拷贝并处理所需的数据段，并让每个拷贝/处理中的数据段均在各自的非默认流中运行。通过使用此技术，您可以开始处理部分数据，同时为后续段并发执行内存传输。使用此技术计算操作次数的数据段特定值和数组内的偏移位置时必须格外小心，如下所示：\n",
    "\n",
    "```cpp\n",
    "int N = 2<<24;\n",
    "int size = N * sizeof(int);\n",
    "\n",
    "int *host_array;\n",
    "int *device_array;\n",
    "\n",
    "cudaMallocHost(&host_array, size);               // Pinned host memory allocation.\n",
    "cudaMalloc(&device_array, size);                 // Allocation directly on the active GPU device.\n",
    "\n",
    "initializeData(host_array, N);                   // Assume this application needs to initialize on the host.\n",
    "\n",
    "const int numberOfSegments = 4;                  // This example demonstrates slicing the work into 4 segments.\n",
    "int segmentN = N / numberOfSegments;             // A value for a segment's worth of `N` is needed.\n",
    "size_t segmentSize = size / numberOfSegments;    // A value for a segment's worth of `size` is needed.\n",
    "\n",
    "// For each of the 4 segments...\n",
    "for (int i = 0; i < numberOfSegments; ++i)\n",
    "{\n",
    "  // Calculate the index where this particular segment should operate within the larger arrays.\n",
    "  segmentOffset = i * segmentN;\n",
    "\n",
    "  // Create a stream for this segment's worth of copy and work.\n",
    "  cudaStream_t stream;\n",
    "  cudaStreamCreate(&stream);\n",
    "  \n",
    "  // Asynchronously copy segment's worth of pinned host memory to device over non-default stream.\n",
    "  cudaMemcpyAsync(&device_array[segmentOffset],  // Take care to access correct location in array.\n",
    "                  &host_array[segmentOffset],    // Take care to access correct location in array.\n",
    "                  segmentSize,                   // Only copy a segment's worth of memory.\n",
    "                  cudaMemcpyHostToDevice,\n",
    "                  stream);                       // Provide optional argument for non-default stream.\n",
    "                  \n",
    "  // Execute segment's worth of work over same non-default stream as memory copy.\n",
    "  kernel<<<number_of_blocks, threads_per_block, 0, stream>>>(&device_array[segmentOffset], segmentN);\n",
    "  \n",
    "  // `cudaStreamDestroy` will return immediately (is non-blocking), but will not actually destroy stream until\n",
    "  // all stream operations are complete.\n",
    "  cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：核函数和内存复制回主机重叠执行\n",
    "\n",
    "向量加法应用程序 [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) 的最新迭代目前正在 GPU 上执行所有向量加法操作，完成后其会将内存拷贝回主机以进行验证。\n",
    "\n",
    "重构 [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) 应用程序，在非默认流中分4段执行矢量加法，以便可以在等待所有向量加法工作完成之前开始异步内存复制。如您遇到问题，请参阅 [解决方案](../edit/08-overlap-xfer/solutions/01-overlap-xfer-solution.cu)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector-add-manual-alloc 07-manual-malloc/solutions/01-manual-malloc-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成重构后，在Nsight Systems中打开一个报告，并使用活动时间表执行以下操作：\n",
    "\n",
    "- 记录设备到主机的内存传输开始时间是在所有核函数工作完成之前还是之后？\n",
    "- 需注意 4 个内存拷贝段本身并不重叠。即使是在单独的非默认流中，在给定方向（此处为 DtoH）上，每次也只能同时进行一个内存传输。此处获得性能提升的原因在于其能先于其他方式开始内存传输，并且不难想象：若在某个应用程序中所完成的工作量与简单的加法运算相比，并非可以几乎忽略不计，那么内存拷贝不仅开始得更早，而且还会与核函数的执行相重叠。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
